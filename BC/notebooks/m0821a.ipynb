{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d658eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn  import metrics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b7533f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train csv\n",
    "zip_dir = '/Data1/Radar'\n",
    "meta_dir = os.path.join(zip_dir, 'meta')\n",
    "x_feature = pd.read_csv(meta_dir+'/x_feature_info.csv')\n",
    "train = pd.read_csv(os.path.join(zip_dir,'train.csv'))\n",
    "test = pd.read_csv(os.path.join(zip_dir,'test.csv'))\n",
    "submission = pd.read_csv(zip_dir+'/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4508f97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lg_nrmse(gt, preds):\n",
    "    # 각 Y Feature별 NRMSE 총합\n",
    "    # Y_01 ~ Y_08 까지 20% 가중치 부여\n",
    "    all_nrmse = []\n",
    "    for idx in range(14): # ignore 'ID'\n",
    "        rmse = metrics.mean_squared_error(gt[:,idx], preds[:,idx], squared=False)\n",
    "        nrmse = rmse/np.mean(np.abs(gt[:,idx]))\n",
    "        all_nrmse.append(nrmse)\n",
    "    score = 1.2 * np.sum(all_nrmse[:8]) + 1.0 * np.sum(all_nrmse[8:14])\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "933a2056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y_01</th>\n",
       "      <th>Y_02</th>\n",
       "      <th>Y_03</th>\n",
       "      <th>Y_04</th>\n",
       "      <th>Y_05</th>\n",
       "      <th>Y_06</th>\n",
       "      <th>Y_07</th>\n",
       "      <th>Y_08</th>\n",
       "      <th>Y_09</th>\n",
       "      <th>Y_10</th>\n",
       "      <th>Y_11</th>\n",
       "      <th>Y_12</th>\n",
       "      <th>Y_13</th>\n",
       "      <th>Y_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.056</td>\n",
       "      <td>1.456</td>\n",
       "      <td>1.680</td>\n",
       "      <td>10.502</td>\n",
       "      <td>29.632</td>\n",
       "      <td>16.083</td>\n",
       "      <td>4.276</td>\n",
       "      <td>-25.381</td>\n",
       "      <td>-25.529</td>\n",
       "      <td>-22.769</td>\n",
       "      <td>23.792</td>\n",
       "      <td>-25.470</td>\n",
       "      <td>-25.409</td>\n",
       "      <td>-25.304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.446</td>\n",
       "      <td>1.184</td>\n",
       "      <td>1.268</td>\n",
       "      <td>18.507</td>\n",
       "      <td>33.179</td>\n",
       "      <td>16.736</td>\n",
       "      <td>3.229</td>\n",
       "      <td>-26.619</td>\n",
       "      <td>-26.523</td>\n",
       "      <td>-22.574</td>\n",
       "      <td>24.691</td>\n",
       "      <td>-26.253</td>\n",
       "      <td>-26.497</td>\n",
       "      <td>-26.438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.251</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.782</td>\n",
       "      <td>14.082</td>\n",
       "      <td>31.801</td>\n",
       "      <td>17.080</td>\n",
       "      <td>2.839</td>\n",
       "      <td>-26.238</td>\n",
       "      <td>-26.216</td>\n",
       "      <td>-22.169</td>\n",
       "      <td>24.649</td>\n",
       "      <td>-26.285</td>\n",
       "      <td>-26.215</td>\n",
       "      <td>-26.370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.464</td>\n",
       "      <td>1.079</td>\n",
       "      <td>1.052</td>\n",
       "      <td>16.975</td>\n",
       "      <td>34.503</td>\n",
       "      <td>17.143</td>\n",
       "      <td>3.144</td>\n",
       "      <td>-25.426</td>\n",
       "      <td>-25.079</td>\n",
       "      <td>-21.765</td>\n",
       "      <td>24.913</td>\n",
       "      <td>-25.254</td>\n",
       "      <td>-25.021</td>\n",
       "      <td>-25.345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.983</td>\n",
       "      <td>0.646</td>\n",
       "      <td>0.689</td>\n",
       "      <td>15.047</td>\n",
       "      <td>32.602</td>\n",
       "      <td>17.569</td>\n",
       "      <td>3.138</td>\n",
       "      <td>-25.376</td>\n",
       "      <td>-25.242</td>\n",
       "      <td>-21.072</td>\n",
       "      <td>25.299</td>\n",
       "      <td>-25.072</td>\n",
       "      <td>-25.195</td>\n",
       "      <td>-24.974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39602</th>\n",
       "      <td>1.382</td>\n",
       "      <td>1.215</td>\n",
       "      <td>1.263</td>\n",
       "      <td>10.874</td>\n",
       "      <td>29.194</td>\n",
       "      <td>16.582</td>\n",
       "      <td>3.410</td>\n",
       "      <td>-26.486</td>\n",
       "      <td>-26.581</td>\n",
       "      <td>-22.772</td>\n",
       "      <td>24.261</td>\n",
       "      <td>-26.491</td>\n",
       "      <td>-26.584</td>\n",
       "      <td>-26.580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39603</th>\n",
       "      <td>1.482</td>\n",
       "      <td>0.606</td>\n",
       "      <td>1.083</td>\n",
       "      <td>8.759</td>\n",
       "      <td>29.859</td>\n",
       "      <td>15.659</td>\n",
       "      <td>3.406</td>\n",
       "      <td>-27.308</td>\n",
       "      <td>-27.203</td>\n",
       "      <td>-24.674</td>\n",
       "      <td>23.427</td>\n",
       "      <td>-27.250</td>\n",
       "      <td>-27.334</td>\n",
       "      <td>-27.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39604</th>\n",
       "      <td>1.117</td>\n",
       "      <td>1.154</td>\n",
       "      <td>0.993</td>\n",
       "      <td>13.159</td>\n",
       "      <td>24.720</td>\n",
       "      <td>16.823</td>\n",
       "      <td>3.215</td>\n",
       "      <td>-26.502</td>\n",
       "      <td>-26.687</td>\n",
       "      <td>-22.577</td>\n",
       "      <td>24.301</td>\n",
       "      <td>-26.388</td>\n",
       "      <td>-26.425</td>\n",
       "      <td>-26.601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39605</th>\n",
       "      <td>0.895</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.477</td>\n",
       "      <td>9.123</td>\n",
       "      <td>26.412</td>\n",
       "      <td>15.757</td>\n",
       "      <td>4.216</td>\n",
       "      <td>-26.760</td>\n",
       "      <td>-26.634</td>\n",
       "      <td>-24.066</td>\n",
       "      <td>23.305</td>\n",
       "      <td>-26.536</td>\n",
       "      <td>-26.751</td>\n",
       "      <td>-26.635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39606</th>\n",
       "      <td>1.147</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.852</td>\n",
       "      <td>10.421</td>\n",
       "      <td>30.745</td>\n",
       "      <td>16.781</td>\n",
       "      <td>3.307</td>\n",
       "      <td>-26.054</td>\n",
       "      <td>-26.251</td>\n",
       "      <td>-23.257</td>\n",
       "      <td>24.450</td>\n",
       "      <td>-26.224</td>\n",
       "      <td>-26.256</td>\n",
       "      <td>-26.093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39607 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Y_01   Y_02   Y_03    Y_04    Y_05    Y_06   Y_07    Y_08    Y_09  \\\n",
       "0      2.056  1.456  1.680  10.502  29.632  16.083  4.276 -25.381 -25.529   \n",
       "1      1.446  1.184  1.268  18.507  33.179  16.736  3.229 -26.619 -26.523   \n",
       "2      1.251  0.665  0.782  14.082  31.801  17.080  2.839 -26.238 -26.216   \n",
       "3      1.464  1.079  1.052  16.975  34.503  17.143  3.144 -25.426 -25.079   \n",
       "4      0.983  0.646  0.689  15.047  32.602  17.569  3.138 -25.376 -25.242   \n",
       "...      ...    ...    ...     ...     ...     ...    ...     ...     ...   \n",
       "39602  1.382  1.215  1.263  10.874  29.194  16.582  3.410 -26.486 -26.581   \n",
       "39603  1.482  0.606  1.083   8.759  29.859  15.659  3.406 -27.308 -27.203   \n",
       "39604  1.117  1.154  0.993  13.159  24.720  16.823  3.215 -26.502 -26.687   \n",
       "39605  0.895  0.187  0.477   9.123  26.412  15.757  4.216 -26.760 -26.634   \n",
       "39606  1.147  0.348  0.852  10.421  30.745  16.781  3.307 -26.054 -26.251   \n",
       "\n",
       "         Y_10    Y_11    Y_12    Y_13    Y_14  \n",
       "0     -22.769  23.792 -25.470 -25.409 -25.304  \n",
       "1     -22.574  24.691 -26.253 -26.497 -26.438  \n",
       "2     -22.169  24.649 -26.285 -26.215 -26.370  \n",
       "3     -21.765  24.913 -25.254 -25.021 -25.345  \n",
       "4     -21.072  25.299 -25.072 -25.195 -24.974  \n",
       "...       ...     ...     ...     ...     ...  \n",
       "39602 -22.772  24.261 -26.491 -26.584 -26.580  \n",
       "39603 -24.674  23.427 -27.250 -27.334 -27.325  \n",
       "39604 -22.577  24.301 -26.388 -26.425 -26.601  \n",
       "39605 -24.066  23.305 -26.536 -26.751 -26.635  \n",
       "39606 -23.257  24.450 -26.224 -26.256 -26.093  \n",
       "\n",
       "[39607 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train.columns[-14:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03bb7a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x : (39607, 56)\n",
      "train_y : (39607, 14)\n"
     ]
    }
   ],
   "source": [
    "train_x = np.array(train[train.columns[1:-14]])\n",
    "print(f'train_x : {train_x.shape}')\n",
    "train_y = np.array(train[train.columns[-14:]])\n",
    "print(f'train_y : {train_y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71f6029f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39607, 56)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train_x)\n",
    "scaled_x = scaler.transform(train_x)\n",
    "scaled_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10a1e7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39607, 14)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = np.array(train_y)\n",
    "label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604c1e6c",
   "metadata": {},
   "source": [
    "# Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a8faaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train : (31685, 56)\n",
      "y_train : (31685, 14)\n",
      "x_test  : (7922, 56)\n",
      "y_test  : (7922, 14)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = tts(scaled_x, label, test_size = 0.2, random_state = 1)\n",
    "print(f'x_train : {x_train.shape}')\n",
    "print(f'y_train : {y_train.shape}')\n",
    "print(f'x_test  : {x_test.shape}')\n",
    "print(f'y_test  : {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa8cef8",
   "metadata": {},
   "source": [
    "# Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4491f92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  \n",
    "  try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[3], 'GPU')\n",
    "  except RuntimeError as e:\n",
    "    \n",
    "    print(e)\n",
    "def get_model() :\n",
    "    inputs = keras.Input(shape=(56,))\n",
    "\n",
    "    x1 = layers.Dense(112, activation = 'swish')(inputs)\n",
    "    x = layers.BatchNormalization()(x1)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = x + x1\n",
    "    \n",
    "    x2 = layers.Dense(60, activation = 'swish')(x)\n",
    "    x = layers.BatchNormalization()(x2)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = x + x2\n",
    "\n",
    "    x3 = layers.Dense(30, activation = 'swish')(x)\n",
    "    x = layers.BatchNormalization()(x3)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = x + x3\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    outputs = layers.Dense(14)(x)\n",
    "\n",
    "    return keras.Model(inputs,outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6feb6aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 11:39:35.407145: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-23 11:39:36.133088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 42866 MB memory:  -> device: 3, name: Quadro RTX 8000, pci bus id: 0000:40:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.compile(loss = 'mse',\n",
    "             optimizer = 'adam',\n",
    "             metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04f10921",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = 'm0821.ckpt',\n",
    "    monitor = 'val_loss',\n",
    "    verbose = 1,\n",
    "    save_best_only = True,\n",
    "    save_weights_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51525693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 11:49:21.792738: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "62/62 [==============================] - 3s 12ms/step - loss: 328.3429 - mse: 328.3429 - val_loss: 90.6308 - val_mse: 90.6308\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 90.63081, saving model to m0821.ckpt\n",
      "Epoch 2/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 102.4407 - mse: 102.4407 - val_loss: 26.5891 - val_mse: 26.5891\n",
      "\n",
      "Epoch 00002: val_loss improved from 90.63081 to 26.58909, saving model to m0821.ckpt\n",
      "Epoch 3/300\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 52.3672 - mse: 52.3672 - val_loss: 17.4560 - val_mse: 17.4560\n",
      "\n",
      "Epoch 00003: val_loss improved from 26.58909 to 17.45601, saving model to m0821.ckpt\n",
      "Epoch 4/300\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 40.1380 - mse: 40.1380 - val_loss: 9.3711 - val_mse: 9.3711\n",
      "\n",
      "Epoch 00004: val_loss improved from 17.45601 to 9.37112, saving model to m0821.ckpt\n",
      "Epoch 5/300\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 33.7123 - mse: 33.7123 - val_loss: 6.9405 - val_mse: 6.9405\n",
      "\n",
      "Epoch 00005: val_loss improved from 9.37112 to 6.94054, saving model to m0821.ckpt\n",
      "Epoch 6/300\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 28.9559 - mse: 28.9559 - val_loss: 4.8727 - val_mse: 4.8727\n",
      "\n",
      "Epoch 00006: val_loss improved from 6.94054 to 4.87273, saving model to m0821.ckpt\n",
      "Epoch 7/300\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 25.7848 - mse: 25.7848 - val_loss: 4.3071 - val_mse: 4.3071\n",
      "\n",
      "Epoch 00007: val_loss improved from 4.87273 to 4.30713, saving model to m0821.ckpt\n",
      "Epoch 8/300\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 22.5555 - mse: 22.5555 - val_loss: 3.8949 - val_mse: 3.8949\n",
      "\n",
      "Epoch 00008: val_loss improved from 4.30713 to 3.89485, saving model to m0821.ckpt\n",
      "Epoch 9/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 20.4575 - mse: 20.4575 - val_loss: 3.6504 - val_mse: 3.6504\n",
      "\n",
      "Epoch 00009: val_loss improved from 3.89485 to 3.65040, saving model to m0821.ckpt\n",
      "Epoch 10/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 18.9501 - mse: 18.9501 - val_loss: 2.7465 - val_mse: 2.7465\n",
      "\n",
      "Epoch 00010: val_loss improved from 3.65040 to 2.74651, saving model to m0821.ckpt\n",
      "Epoch 11/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 17.2794 - mse: 17.2794 - val_loss: 2.7680 - val_mse: 2.7680\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.74651\n",
      "Epoch 12/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 15.7378 - mse: 15.7378 - val_loss: 2.3772 - val_mse: 2.3772\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.74651 to 2.37723, saving model to m0821.ckpt\n",
      "Epoch 13/300\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 14.8120 - mse: 14.8120 - val_loss: 2.2439 - val_mse: 2.2439\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.37723 to 2.24388, saving model to m0821.ckpt\n",
      "Epoch 14/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 13.8608 - mse: 13.8608 - val_loss: 2.4837 - val_mse: 2.4837\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2.24388\n",
      "Epoch 15/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 13.1461 - mse: 13.1461 - val_loss: 1.9431 - val_mse: 1.9431\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.24388 to 1.94314, saving model to m0821.ckpt\n",
      "Epoch 16/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 12.5510 - mse: 12.5510 - val_loss: 1.8650 - val_mse: 1.8650\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.94314 to 1.86498, saving model to m0821.ckpt\n",
      "Epoch 17/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 12.0219 - mse: 12.0219 - val_loss: 1.8863 - val_mse: 1.8863\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.86498\n",
      "Epoch 18/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 11.4498 - mse: 11.4498 - val_loss: 1.7731 - val_mse: 1.7731\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.86498 to 1.77315, saving model to m0821.ckpt\n",
      "Epoch 19/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 11.0420 - mse: 11.0420 - val_loss: 1.8427 - val_mse: 1.8427\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.77315\n",
      "Epoch 20/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 10.8305 - mse: 10.8305 - val_loss: 2.1317 - val_mse: 2.1317\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.77315\n",
      "Epoch 21/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 10.4772 - mse: 10.4772 - val_loss: 1.7809 - val_mse: 1.7809\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.77315\n",
      "Epoch 22/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 10.1969 - mse: 10.1969 - val_loss: 1.7241 - val_mse: 1.7241\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.77315 to 1.72411, saving model to m0821.ckpt\n",
      "Epoch 23/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 9.9934 - mse: 9.9934 - val_loss: 2.0227 - val_mse: 2.0227\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.72411\n",
      "Epoch 24/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 9.9539 - mse: 9.9539 - val_loss: 1.9920 - val_mse: 1.9920\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.72411\n",
      "Epoch 25/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 9.7585 - mse: 9.7585 - val_loss: 1.6494 - val_mse: 1.6494\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.72411 to 1.64942, saving model to m0821.ckpt\n",
      "Epoch 26/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 9.5317 - mse: 9.5317 - val_loss: 1.8080 - val_mse: 1.8080\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.64942\n",
      "Epoch 27/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 9.5021 - mse: 9.5021 - val_loss: 1.8249 - val_mse: 1.8249\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.64942\n",
      "Epoch 28/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 9.3970 - mse: 9.3970 - val_loss: 1.8235 - val_mse: 1.8235\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.64942\n",
      "Epoch 29/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 9.3220 - mse: 9.3220 - val_loss: 1.7801 - val_mse: 1.7801\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.64942\n",
      "Epoch 30/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 9.1764 - mse: 9.1764 - val_loss: 1.9898 - val_mse: 1.9898\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.64942\n",
      "Epoch 31/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 9.0359 - mse: 9.0359 - val_loss: 1.6282 - val_mse: 1.6282\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.64942 to 1.62824, saving model to m0821.ckpt\n",
      "Epoch 32/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 8.9836 - mse: 8.9836 - val_loss: 1.6745 - val_mse: 1.6745\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.62824\n",
      "Epoch 33/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 8.8047 - mse: 8.8047 - val_loss: 1.7761 - val_mse: 1.7761\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.62824\n",
      "Epoch 34/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 8.8795 - mse: 8.8795 - val_loss: 1.9540 - val_mse: 1.9540\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.62824\n",
      "Epoch 35/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 8.6934 - mse: 8.6934 - val_loss: 1.6784 - val_mse: 1.6784\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.62824\n",
      "Epoch 36/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 8.6389 - mse: 8.6389 - val_loss: 1.5888 - val_mse: 1.5888\n",
      "\n",
      "Epoch 00036: val_loss improved from 1.62824 to 1.58884, saving model to m0821.ckpt\n",
      "Epoch 37/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 8.6474 - mse: 8.6474 - val_loss: 1.6913 - val_mse: 1.6913\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.58884\n",
      "Epoch 38/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 8.4274 - mse: 8.4274 - val_loss: 1.5979 - val_mse: 1.5979\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.58884\n",
      "Epoch 39/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 8.4232 - mse: 8.4232 - val_loss: 1.7813 - val_mse: 1.7813\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.58884\n",
      "Epoch 40/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 8.3226 - mse: 8.3226 - val_loss: 1.6961 - val_mse: 1.6961\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.58884\n",
      "Epoch 41/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 8.4222 - mse: 8.4222 - val_loss: 1.6144 - val_mse: 1.6144\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.58884\n",
      "Epoch 42/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 8.2788 - mse: 8.2788 - val_loss: 1.5898 - val_mse: 1.5898\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.58884\n",
      "Epoch 43/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 8.2369 - mse: 8.2369 - val_loss: 1.5519 - val_mse: 1.5519\n",
      "\n",
      "Epoch 00043: val_loss improved from 1.58884 to 1.55191, saving model to m0821.ckpt\n",
      "Epoch 44/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 8.1450 - mse: 8.1450 - val_loss: 1.6673 - val_mse: 1.6673\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.55191\n",
      "Epoch 45/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 8.1473 - mse: 8.1473 - val_loss: 1.6077 - val_mse: 1.6077\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.55191\n",
      "Epoch 46/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 8.1960 - mse: 8.1960 - val_loss: 1.5695 - val_mse: 1.5695\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.55191\n",
      "Epoch 47/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 8.2341 - mse: 8.2341 - val_loss: 1.5811 - val_mse: 1.5811\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.55191\n",
      "Epoch 48/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 8.1674 - mse: 8.1674 - val_loss: 1.5953 - val_mse: 1.5953\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.55191\n",
      "Epoch 49/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 8.1099 - mse: 8.1099 - val_loss: 1.6286 - val_mse: 1.6286\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.55191\n",
      "Epoch 50/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 8.1020 - mse: 8.1020 - val_loss: 1.5847 - val_mse: 1.5847\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.55191\n",
      "Epoch 51/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 7.9846 - mse: 7.9846 - val_loss: 1.5591 - val_mse: 1.5591\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1.55191\n",
      "Epoch 52/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 8.0557 - mse: 8.0557 - val_loss: 1.5523 - val_mse: 1.5523\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.55191\n",
      "Epoch 53/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 7.9721 - mse: 7.9721 - val_loss: 1.5996 - val_mse: 1.5996\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.55191\n",
      "Epoch 54/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 7.9660 - mse: 7.9660 - val_loss: 1.6120 - val_mse: 1.6120\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1.55191\n",
      "Epoch 55/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 7.9149 - mse: 7.9149 - val_loss: 1.5811 - val_mse: 1.5811\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1.55191\n",
      "Epoch 56/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 7.9277 - mse: 7.9277 - val_loss: 1.5905 - val_mse: 1.5905\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 1.55191\n",
      "Epoch 57/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 7.8182 - mse: 7.8182 - val_loss: 1.7572 - val_mse: 1.7572\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1.55191\n",
      "Epoch 58/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 7.9261 - mse: 7.9261 - val_loss: 1.5625 - val_mse: 1.5625\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.55191\n",
      "Epoch 59/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 7.8491 - mse: 7.8491 - val_loss: 1.6901 - val_mse: 1.6901\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.55191\n",
      "Epoch 60/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 7.9060 - mse: 7.9060 - val_loss: 1.5747 - val_mse: 1.5747\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1.55191\n",
      "Epoch 61/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 7.8391 - mse: 7.8391 - val_loss: 1.5865 - val_mse: 1.5865\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1.55191\n",
      "Epoch 62/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 7.8115 - mse: 7.8115 - val_loss: 1.6043 - val_mse: 1.6043\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 1.55191\n",
      "Epoch 63/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 7.7397 - mse: 7.7397 - val_loss: 1.6369 - val_mse: 1.6369\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 1.55191\n",
      "Epoch 64/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 7.7399 - mse: 7.7399 - val_loss: 1.6917 - val_mse: 1.6917\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 1.55191\n",
      "Epoch 65/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 7.8024 - mse: 7.8024 - val_loss: 1.6489 - val_mse: 1.6489\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 1.55191\n",
      "Epoch 66/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 7.6821 - mse: 7.6821 - val_loss: 1.6439 - val_mse: 1.6439\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 1.55191\n",
      "Epoch 67/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 7.7580 - mse: 7.7580 - val_loss: 1.5816 - val_mse: 1.5816\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1.55191\n",
      "Epoch 68/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 7.7353 - mse: 7.7353 - val_loss: 1.6264 - val_mse: 1.6264\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1.55191\n",
      "Epoch 69/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 7.8016 - mse: 7.8016 - val_loss: 1.6583 - val_mse: 1.6583\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 1.55191\n",
      "Epoch 70/300\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 7.6322 - mse: 7.6322 - val_loss: 1.6428 - val_mse: 1.6428\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1.55191\n",
      "Epoch 71/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 7.5988 - mse: 7.5988 - val_loss: 1.7106 - val_mse: 1.7106\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1.55191\n",
      "Epoch 72/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 7.6835 - mse: 7.6835 - val_loss: 1.6087 - val_mse: 1.6087\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 1.55191\n",
      "Epoch 73/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 7.5825 - mse: 7.5825 - val_loss: 1.6289 - val_mse: 1.6289\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 1.55191\n",
      "Epoch 74/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 7.4964 - mse: 7.4964 - val_loss: 1.6838 - val_mse: 1.6838\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1.55191\n",
      "Epoch 75/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 7.5881 - mse: 7.5881 - val_loss: 1.6843 - val_mse: 1.6843\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 1.55191\n",
      "Epoch 76/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 7.5143 - mse: 7.5143 - val_loss: 1.7160 - val_mse: 1.7160\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 1.55191\n",
      "Epoch 77/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 7.4879 - mse: 7.4879 - val_loss: 1.7234 - val_mse: 1.7234\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 1.55191\n",
      "Epoch 78/300\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 7.3786 - mse: 7.3786 - val_loss: 1.5943 - val_mse: 1.5943\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 1.55191\n",
      "Epoch 79/300\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 7.4903 - mse: 7.4903 - val_loss: 1.6168 - val_mse: 1.6168\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 1.55191\n",
      "Epoch 80/300\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 7.5414 - mse: 7.5414 - val_loss: 1.5747 - val_mse: 1.5747\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1.55191\n",
      "Epoch 81/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 7.4956 - mse: 7.4956 - val_loss: 1.6630 - val_mse: 1.6630\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1.55191\n",
      "Epoch 82/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 7.4364 - mse: 7.4364 - val_loss: 1.7042 - val_mse: 1.7042\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1.55191\n",
      "Epoch 83/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 7.3564 - mse: 7.3564 - val_loss: 1.6713 - val_mse: 1.6713\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 1.55191\n",
      "Epoch 84/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 7.3890 - mse: 7.3890 - val_loss: 1.6264 - val_mse: 1.6264\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 1.55191\n",
      "Epoch 85/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 7.3884 - mse: 7.3884 - val_loss: 1.5922 - val_mse: 1.5922\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 1.55191\n",
      "Epoch 86/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 7.4079 - mse: 7.4079 - val_loss: 1.5821 - val_mse: 1.5821\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 1.55191\n",
      "Epoch 87/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 7.2138 - mse: 7.2138 - val_loss: 1.6806 - val_mse: 1.6806\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 1.55191\n",
      "Epoch 88/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 7.3003 - mse: 7.3003 - val_loss: 1.6229 - val_mse: 1.6229\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 1.55191\n",
      "Epoch 89/300\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 7.2406 - mse: 7.2406 - val_loss: 1.5937 - val_mse: 1.5937\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 1.55191\n",
      "Epoch 90/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 7.2635 - mse: 7.2635 - val_loss: 1.5971 - val_mse: 1.5971\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 1.55191\n",
      "Epoch 91/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 7.2609 - mse: 7.2609 - val_loss: 1.5868 - val_mse: 1.5868\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 1.55191\n",
      "Epoch 92/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 7.2173 - mse: 7.2173 - val_loss: 1.6208 - val_mse: 1.6208\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 1.55191\n",
      "Epoch 93/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 7.2396 - mse: 7.2396 - val_loss: 1.6207 - val_mse: 1.6207\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 1.55191\n",
      "Epoch 94/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 7.2254 - mse: 7.2254 - val_loss: 1.5632 - val_mse: 1.5632\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 1.55191\n",
      "Epoch 95/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 7.1732 - mse: 7.1732 - val_loss: 1.5423 - val_mse: 1.5423\n",
      "\n",
      "Epoch 00095: val_loss improved from 1.55191 to 1.54227, saving model to m0821.ckpt\n",
      "Epoch 96/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 7.0628 - mse: 7.0628 - val_loss: 1.5390 - val_mse: 1.5390\n",
      "\n",
      "Epoch 00096: val_loss improved from 1.54227 to 1.53898, saving model to m0821.ckpt\n",
      "Epoch 97/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 7.0445 - mse: 7.0445 - val_loss: 1.5506 - val_mse: 1.5506\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 1.53898\n",
      "Epoch 98/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 7.0953 - mse: 7.0953 - val_loss: 1.5731 - val_mse: 1.5731\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 1.53898\n",
      "Epoch 99/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 7.0921 - mse: 7.0921 - val_loss: 1.6270 - val_mse: 1.6270\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 1.53898\n",
      "Epoch 100/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 6.9380 - mse: 6.9380 - val_loss: 1.5616 - val_mse: 1.5616\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 1.53898\n",
      "Epoch 101/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 7.0755 - mse: 7.0755 - val_loss: 1.6452 - val_mse: 1.6452\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 1.53898\n",
      "Epoch 102/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 6.9622 - mse: 6.9622 - val_loss: 1.5926 - val_mse: 1.5926\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 1.53898\n",
      "Epoch 103/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 7.0460 - mse: 7.0460 - val_loss: 1.6022 - val_mse: 1.6022\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 1.53898\n",
      "Epoch 104/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 6.9433 - mse: 6.9433 - val_loss: 1.6100 - val_mse: 1.6100\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 1.53898\n",
      "Epoch 105/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 6.8109 - mse: 6.8109 - val_loss: 1.6039 - val_mse: 1.6039\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 1.53898\n",
      "Epoch 106/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 6.7736 - mse: 6.7736 - val_loss: 1.5807 - val_mse: 1.5807\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 1.53898\n",
      "Epoch 107/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 6.7478 - mse: 6.7478 - val_loss: 1.6011 - val_mse: 1.6011\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 1.53898\n",
      "Epoch 108/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 6.8557 - mse: 6.8557 - val_loss: 1.6811 - val_mse: 1.6811\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 1.53898\n",
      "Epoch 109/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 6.8368 - mse: 6.8368 - val_loss: 1.5936 - val_mse: 1.5936\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 1.53898\n",
      "Epoch 110/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 6.7606 - mse: 6.7606 - val_loss: 1.6105 - val_mse: 1.6105\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 1.53898\n",
      "Epoch 111/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 6.8173 - mse: 6.8173 - val_loss: 1.6070 - val_mse: 1.6070\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 1.53898\n",
      "Epoch 112/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 6.6540 - mse: 6.6540 - val_loss: 1.6381 - val_mse: 1.6381\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 1.53898\n",
      "Epoch 113/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 6.7298 - mse: 6.7298 - val_loss: 1.6285 - val_mse: 1.6285\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 1.53898\n",
      "Epoch 114/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 6.6682 - mse: 6.6682 - val_loss: 1.6041 - val_mse: 1.6041\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 1.53898\n",
      "Epoch 115/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 6.6975 - mse: 6.6975 - val_loss: 1.6463 - val_mse: 1.6463\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 1.53898\n",
      "Epoch 116/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 6.6153 - mse: 6.6153 - val_loss: 1.5912 - val_mse: 1.5912\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 1.53898\n",
      "Epoch 117/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 6.5332 - mse: 6.5332 - val_loss: 1.5759 - val_mse: 1.5759\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 1.53898\n",
      "Epoch 118/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 6.6127 - mse: 6.6127 - val_loss: 1.5973 - val_mse: 1.5973\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 1.53898\n",
      "Epoch 119/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 6.5796 - mse: 6.5796 - val_loss: 1.6701 - val_mse: 1.6701\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 1.53898\n",
      "Epoch 120/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 6.6276 - mse: 6.6276 - val_loss: 1.6391 - val_mse: 1.6391\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 1.53898\n",
      "Epoch 121/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 6.5096 - mse: 6.5096 - val_loss: 1.6583 - val_mse: 1.6583\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 1.53898\n",
      "Epoch 122/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 6.5321 - mse: 6.5321 - val_loss: 1.5863 - val_mse: 1.5863\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 1.53898\n",
      "Epoch 123/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 6.4302 - mse: 6.4302 - val_loss: 1.5993 - val_mse: 1.5993\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 1.53898\n",
      "Epoch 124/300\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 6.4003 - mse: 6.4003 - val_loss: 1.5878 - val_mse: 1.5878\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 1.53898\n",
      "Epoch 125/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 6.4298 - mse: 6.4298 - val_loss: 1.5766 - val_mse: 1.5766\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 1.53898\n",
      "Epoch 126/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 6.4006 - mse: 6.4006 - val_loss: 1.6241 - val_mse: 1.6241\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 1.53898\n",
      "Epoch 127/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 6.3075 - mse: 6.3075 - val_loss: 1.5979 - val_mse: 1.5979\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 1.53898\n",
      "Epoch 128/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 6.3012 - mse: 6.3012 - val_loss: 1.5832 - val_mse: 1.5832\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 1.53898\n",
      "Epoch 129/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 6.3339 - mse: 6.3339 - val_loss: 1.6371 - val_mse: 1.6371\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 1.53898\n",
      "Epoch 130/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 6.3339 - mse: 6.3339 - val_loss: 1.6519 - val_mse: 1.6519\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 1.53898\n",
      "Epoch 131/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 6.3145 - mse: 6.3145 - val_loss: 1.5797 - val_mse: 1.5797\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 1.53898\n",
      "Epoch 132/300\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 6.2677 - mse: 6.2677 - val_loss: 1.6406 - val_mse: 1.6406\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 1.53898\n",
      "Epoch 133/300\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 6.2256 - mse: 6.2256 - val_loss: 1.5586 - val_mse: 1.5586\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 1.53898\n",
      "Epoch 134/300\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 6.2050 - mse: 6.2050 - val_loss: 1.6161 - val_mse: 1.6161\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 1.53898\n",
      "Epoch 135/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 6.1886 - mse: 6.1886 - val_loss: 1.5968 - val_mse: 1.5968\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 1.53898\n",
      "Epoch 136/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 6.1270 - mse: 6.1270 - val_loss: 1.6098 - val_mse: 1.6098\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 1.53898\n",
      "Epoch 137/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 6.1764 - mse: 6.1764 - val_loss: 1.5806 - val_mse: 1.5806\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 1.53898\n",
      "Epoch 138/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 6.1180 - mse: 6.1180 - val_loss: 1.6047 - val_mse: 1.6047\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 1.53898\n",
      "Epoch 139/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 6.0860 - mse: 6.0860 - val_loss: 1.5830 - val_mse: 1.5830\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 1.53898\n",
      "Epoch 140/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 6.0168 - mse: 6.0168 - val_loss: 1.5952 - val_mse: 1.5952\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 1.53898\n",
      "Epoch 141/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 6.0667 - mse: 6.0667 - val_loss: 1.5852 - val_mse: 1.5852\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 1.53898\n",
      "Epoch 142/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 6.0753 - mse: 6.0753 - val_loss: 1.6152 - val_mse: 1.6152\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 1.53898\n",
      "Epoch 143/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 5.9482 - mse: 5.9482 - val_loss: 1.6014 - val_mse: 1.6014\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 1.53898\n",
      "Epoch 144/300\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 5.9841 - mse: 5.9841 - val_loss: 1.5634 - val_mse: 1.5634\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 1.53898\n",
      "Epoch 145/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 5.9079 - mse: 5.9079 - val_loss: 1.5447 - val_mse: 1.5447\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 1.53898\n",
      "Epoch 146/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 5.9507 - mse: 5.9507 - val_loss: 1.5829 - val_mse: 1.5829\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 1.53898\n",
      "Epoch 147/300\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 5.8656 - mse: 5.8656 - val_loss: 1.5559 - val_mse: 1.5559\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 1.53898\n",
      "Epoch 148/300\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 5.8436 - mse: 5.8436 - val_loss: 1.6099 - val_mse: 1.6099\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 1.53898\n",
      "Epoch 149/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 5.8477 - mse: 5.8477 - val_loss: 1.5329 - val_mse: 1.5329\n",
      "\n",
      "Epoch 00149: val_loss improved from 1.53898 to 1.53292, saving model to m0821.ckpt\n",
      "Epoch 150/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 5.8101 - mse: 5.8101 - val_loss: 1.5344 - val_mse: 1.5344\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 1.53292\n",
      "Epoch 151/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 5.8419 - mse: 5.8419 - val_loss: 1.5307 - val_mse: 1.5307\n",
      "\n",
      "Epoch 00151: val_loss improved from 1.53292 to 1.53074, saving model to m0821.ckpt\n",
      "Epoch 152/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 5.7440 - mse: 5.7440 - val_loss: 1.5372 - val_mse: 1.5372\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 1.53074\n",
      "Epoch 153/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 5.7116 - mse: 5.7116 - val_loss: 1.5520 - val_mse: 1.5520\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 1.53074\n",
      "Epoch 154/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 5.7575 - mse: 5.7575 - val_loss: 1.5538 - val_mse: 1.5538\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 1.53074\n",
      "Epoch 155/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 5.7294 - mse: 5.7294 - val_loss: 1.5503 - val_mse: 1.5503\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 1.53074\n",
      "Epoch 156/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 5.6073 - mse: 5.6073 - val_loss: 1.5697 - val_mse: 1.5697\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 1.53074\n",
      "Epoch 157/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 5.5900 - mse: 5.5900 - val_loss: 1.5943 - val_mse: 1.5943\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 1.53074\n",
      "Epoch 158/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 5.6659 - mse: 5.6659 - val_loss: 1.5875 - val_mse: 1.5875\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 1.53074\n",
      "Epoch 159/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 5.6361 - mse: 5.6361 - val_loss: 1.5878 - val_mse: 1.5878\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 1.53074\n",
      "Epoch 160/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 5.5591 - mse: 5.5591 - val_loss: 1.5727 - val_mse: 1.5727\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 1.53074\n",
      "Epoch 161/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 5.4888 - mse: 5.4888 - val_loss: 1.5770 - val_mse: 1.5770\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 1.53074\n",
      "Epoch 162/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 5.5511 - mse: 5.5511 - val_loss: 1.5658 - val_mse: 1.5658\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 1.53074\n",
      "Epoch 163/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 5.5205 - mse: 5.5205 - val_loss: 1.5633 - val_mse: 1.5633\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 1.53074\n",
      "Epoch 164/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 5.4705 - mse: 5.4705 - val_loss: 1.6355 - val_mse: 1.6355\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 1.53074\n",
      "Epoch 165/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 5.5001 - mse: 5.5001 - val_loss: 1.5749 - val_mse: 1.5749\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 1.53074\n",
      "Epoch 166/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 5.4084 - mse: 5.4084 - val_loss: 1.5870 - val_mse: 1.5870\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 1.53074\n",
      "Epoch 167/300\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 5.4202 - mse: 5.4202 - val_loss: 1.5575 - val_mse: 1.5575\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 1.53074\n",
      "Epoch 168/300\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 5.3913 - mse: 5.3913 - val_loss: 1.5840 - val_mse: 1.5840\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 1.53074\n",
      "Epoch 169/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 5.4066 - mse: 5.4066 - val_loss: 1.5847 - val_mse: 1.5847\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 1.53074\n",
      "Epoch 170/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 5.3638 - mse: 5.3638 - val_loss: 1.6044 - val_mse: 1.6044\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 1.53074\n",
      "Epoch 171/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 5.3691 - mse: 5.3691 - val_loss: 1.5784 - val_mse: 1.5784\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 1.53074\n",
      "Epoch 172/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 5.3490 - mse: 5.3490 - val_loss: 1.5756 - val_mse: 1.5756\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 1.53074\n",
      "Epoch 173/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 5.2591 - mse: 5.2591 - val_loss: 1.5971 - val_mse: 1.5971\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 1.53074\n",
      "Epoch 174/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 5.2999 - mse: 5.2999 - val_loss: 1.5518 - val_mse: 1.5518\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 1.53074\n",
      "Epoch 175/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 5.3108 - mse: 5.3108 - val_loss: 1.5850 - val_mse: 1.5850\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 1.53074\n",
      "Epoch 176/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 5.2849 - mse: 5.2849 - val_loss: 1.5768 - val_mse: 1.5768\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 1.53074\n",
      "Epoch 177/300\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 5.2490 - mse: 5.2490 - val_loss: 1.5733 - val_mse: 1.5733\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 1.53074\n",
      "Epoch 178/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 5.1939 - mse: 5.1939 - val_loss: 1.5631 - val_mse: 1.5631\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 1.53074\n",
      "Epoch 179/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 5.2334 - mse: 5.2334 - val_loss: 1.5765 - val_mse: 1.5765\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 1.53074\n",
      "Epoch 180/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 5.1636 - mse: 5.1636 - val_loss: 1.5755 - val_mse: 1.5755\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 1.53074\n",
      "Epoch 181/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 5.1612 - mse: 5.1612 - val_loss: 1.5524 - val_mse: 1.5524\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 1.53074\n",
      "Epoch 182/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 5.1455 - mse: 5.1455 - val_loss: 1.5770 - val_mse: 1.5770\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 1.53074\n",
      "Epoch 183/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 5.1007 - mse: 5.1007 - val_loss: 1.5762 - val_mse: 1.5762\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 1.53074\n",
      "Epoch 184/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 5.0964 - mse: 5.0964 - val_loss: 1.5724 - val_mse: 1.5724\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 1.53074\n",
      "Epoch 185/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 5.0626 - mse: 5.0626 - val_loss: 1.5813 - val_mse: 1.5813\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 1.53074\n",
      "Epoch 186/300\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 5.0631 - mse: 5.0631 - val_loss: 1.5896 - val_mse: 1.5896\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 1.53074\n",
      "Epoch 187/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 5.0317 - mse: 5.0317 - val_loss: 1.5495 - val_mse: 1.5495\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 1.53074\n",
      "Epoch 188/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 5.0652 - mse: 5.0652 - val_loss: 1.5792 - val_mse: 1.5792\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 1.53074\n",
      "Epoch 189/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.9762 - mse: 4.9762 - val_loss: 1.5660 - val_mse: 1.5660\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 1.53074\n",
      "Epoch 190/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.9351 - mse: 4.9351 - val_loss: 1.5622 - val_mse: 1.5622\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 1.53074\n",
      "Epoch 191/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.9102 - mse: 4.9102 - val_loss: 1.5729 - val_mse: 1.5729\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 1.53074\n",
      "Epoch 192/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.9174 - mse: 4.9174 - val_loss: 1.5633 - val_mse: 1.5633\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 1.53074\n",
      "Epoch 193/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.8972 - mse: 4.8972 - val_loss: 1.5505 - val_mse: 1.5505\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 1.53074\n",
      "Epoch 194/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.8127 - mse: 4.8127 - val_loss: 1.5839 - val_mse: 1.5839\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 1.53074\n",
      "Epoch 195/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.8151 - mse: 4.8151 - val_loss: 1.5618 - val_mse: 1.5618\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 1.53074\n",
      "Epoch 196/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.7994 - mse: 4.7994 - val_loss: 1.5889 - val_mse: 1.5889\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 1.53074\n",
      "Epoch 197/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.8405 - mse: 4.8405 - val_loss: 1.5513 - val_mse: 1.5513\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 1.53074\n",
      "Epoch 198/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.7942 - mse: 4.7942 - val_loss: 1.5693 - val_mse: 1.5693\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 1.53074\n",
      "Epoch 199/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.7735 - mse: 4.7735 - val_loss: 1.5682 - val_mse: 1.5682\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 1.53074\n",
      "Epoch 200/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.7480 - mse: 4.7480 - val_loss: 1.5831 - val_mse: 1.5831\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 1.53074\n",
      "Epoch 201/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.7767 - mse: 4.7767 - val_loss: 1.5961 - val_mse: 1.5961\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 1.53074\n",
      "Epoch 202/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.7161 - mse: 4.7161 - val_loss: 1.5788 - val_mse: 1.5788\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 1.53074\n",
      "Epoch 203/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.7207 - mse: 4.7207 - val_loss: 1.5827 - val_mse: 1.5827\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 1.53074\n",
      "Epoch 204/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.6415 - mse: 4.6415 - val_loss: 1.5719 - val_mse: 1.5719\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 1.53074\n",
      "Epoch 205/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.6587 - mse: 4.6587 - val_loss: 1.5913 - val_mse: 1.5913\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 1.53074\n",
      "Epoch 206/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.6529 - mse: 4.6529 - val_loss: 1.5699 - val_mse: 1.5699\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 1.53074\n",
      "Epoch 207/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.6374 - mse: 4.6374 - val_loss: 1.5495 - val_mse: 1.5495\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 1.53074\n",
      "Epoch 208/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.6139 - mse: 4.6139 - val_loss: 1.5838 - val_mse: 1.5838\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 1.53074\n",
      "Epoch 209/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.5687 - mse: 4.5687 - val_loss: 1.5767 - val_mse: 1.5767\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 1.53074\n",
      "Epoch 210/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.6349 - mse: 4.6349 - val_loss: 1.5635 - val_mse: 1.5635\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 1.53074\n",
      "Epoch 211/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.5562 - mse: 4.5562 - val_loss: 1.5586 - val_mse: 1.5586\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 1.53074\n",
      "Epoch 212/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 4.5112 - mse: 4.5112 - val_loss: 1.5508 - val_mse: 1.5508\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 1.53074\n",
      "Epoch 213/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.5530 - mse: 4.5530 - val_loss: 1.5739 - val_mse: 1.5739\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 1.53074\n",
      "Epoch 214/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.5091 - mse: 4.5091 - val_loss: 1.5380 - val_mse: 1.5380\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 1.53074\n",
      "Epoch 215/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.5013 - mse: 4.5013 - val_loss: 1.5436 - val_mse: 1.5436\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 1.53074\n",
      "Epoch 216/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.4811 - mse: 4.4811 - val_loss: 1.5778 - val_mse: 1.5778\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 1.53074\n",
      "Epoch 217/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.4920 - mse: 4.4920 - val_loss: 1.5474 - val_mse: 1.5474\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 1.53074\n",
      "Epoch 218/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.4188 - mse: 4.4188 - val_loss: 1.5564 - val_mse: 1.5564\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 1.53074\n",
      "Epoch 219/300\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 4.4387 - mse: 4.4387 - val_loss: 1.5422 - val_mse: 1.5422\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 1.53074\n",
      "Epoch 220/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.3951 - mse: 4.3951 - val_loss: 1.5693 - val_mse: 1.5693\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 1.53074\n",
      "Epoch 221/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.3805 - mse: 4.3805 - val_loss: 1.5451 - val_mse: 1.5451\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 1.53074\n",
      "Epoch 222/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.3332 - mse: 4.3332 - val_loss: 1.5549 - val_mse: 1.5549\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 1.53074\n",
      "Epoch 223/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.3408 - mse: 4.3408 - val_loss: 1.5564 - val_mse: 1.5564\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 1.53074\n",
      "Epoch 224/300\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 4.3573 - mse: 4.3573 - val_loss: 1.5734 - val_mse: 1.5734\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 1.53074\n",
      "Epoch 225/300\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 4.3231 - mse: 4.3231 - val_loss: 1.5632 - val_mse: 1.5632\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 1.53074\n",
      "Epoch 226/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 4.2969 - mse: 4.2969 - val_loss: 1.5556 - val_mse: 1.5556\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 1.53074\n",
      "Epoch 227/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 4.3022 - mse: 4.3022 - val_loss: 1.5871 - val_mse: 1.5871\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 1.53074\n",
      "Epoch 228/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 4.2639 - mse: 4.2639 - val_loss: 1.5429 - val_mse: 1.5429\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 1.53074\n",
      "Epoch 229/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 4.2202 - mse: 4.2202 - val_loss: 1.5447 - val_mse: 1.5447\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 1.53074\n",
      "Epoch 230/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 4.2373 - mse: 4.2373 - val_loss: 1.5516 - val_mse: 1.5516\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 1.53074\n",
      "Epoch 231/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 4.2343 - mse: 4.2343 - val_loss: 1.5625 - val_mse: 1.5625\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 1.53074\n",
      "Epoch 232/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.1989 - mse: 4.1989 - val_loss: 1.5562 - val_mse: 1.5562\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 1.53074\n",
      "Epoch 233/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.1834 - mse: 4.1834 - val_loss: 1.5612 - val_mse: 1.5612\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 1.53074\n",
      "Epoch 234/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.1957 - mse: 4.1957 - val_loss: 1.5557 - val_mse: 1.5557\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 1.53074\n",
      "Epoch 235/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 4.1910 - mse: 4.1910 - val_loss: 1.5866 - val_mse: 1.5866\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 1.53074\n",
      "Epoch 236/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.1452 - mse: 4.1452 - val_loss: 1.5504 - val_mse: 1.5504\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 1.53074\n",
      "Epoch 237/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 4.1457 - mse: 4.1457 - val_loss: 1.5540 - val_mse: 1.5540\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 1.53074\n",
      "Epoch 238/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.1120 - mse: 4.1120 - val_loss: 1.5492 - val_mse: 1.5492\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 1.53074\n",
      "Epoch 239/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 4.1200 - mse: 4.1200 - val_loss: 1.5636 - val_mse: 1.5636\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 1.53074\n",
      "Epoch 240/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 4.1060 - mse: 4.1060 - val_loss: 1.5405 - val_mse: 1.5405\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 1.53074\n",
      "Epoch 241/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 4.0666 - mse: 4.0666 - val_loss: 1.5437 - val_mse: 1.5437\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 1.53074\n",
      "Epoch 242/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 4.0583 - mse: 4.0583 - val_loss: 1.5753 - val_mse: 1.5753\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 1.53074\n",
      "Epoch 243/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.0195 - mse: 4.0195 - val_loss: 1.5395 - val_mse: 1.5395\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 1.53074\n",
      "Epoch 244/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.0226 - mse: 4.0226 - val_loss: 1.5497 - val_mse: 1.5497\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 1.53074\n",
      "Epoch 245/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.0473 - mse: 4.0473 - val_loss: 1.5354 - val_mse: 1.5354\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 1.53074\n",
      "Epoch 246/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.0111 - mse: 4.0111 - val_loss: 1.5436 - val_mse: 1.5436\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 1.53074\n",
      "Epoch 247/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 3.9863 - mse: 3.9863 - val_loss: 1.5538 - val_mse: 1.5538\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 1.53074\n",
      "Epoch 248/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 3.9552 - mse: 3.9552 - val_loss: 1.5620 - val_mse: 1.5620\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 1.53074\n",
      "Epoch 249/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 3.9331 - mse: 3.9331 - val_loss: 1.5691 - val_mse: 1.5691\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 1.53074\n",
      "Epoch 250/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 3.9065 - mse: 3.9065 - val_loss: 1.5563 - val_mse: 1.5563\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 1.53074\n",
      "Epoch 251/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 3.8903 - mse: 3.8903 - val_loss: 1.5514 - val_mse: 1.5514\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 1.53074\n",
      "Epoch 252/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 3.9140 - mse: 3.9140 - val_loss: 1.5475 - val_mse: 1.5475\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 1.53074\n",
      "Epoch 253/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 3.9024 - mse: 3.9024 - val_loss: 1.5545 - val_mse: 1.5545\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 1.53074\n",
      "Epoch 254/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 3.8684 - mse: 3.8684 - val_loss: 1.5362 - val_mse: 1.5362\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 1.53074\n",
      "Epoch 255/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 3.8579 - mse: 3.8579 - val_loss: 1.5495 - val_mse: 1.5495\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 1.53074\n",
      "Epoch 256/300\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 3.8531 - mse: 3.8531 - val_loss: 1.5553 - val_mse: 1.5553\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 1.53074\n",
      "Epoch 257/300\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 3.8363 - mse: 3.8363 - val_loss: 1.5644 - val_mse: 1.5644\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 1.53074\n",
      "Epoch 258/300\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 3.8182 - mse: 3.8182 - val_loss: 1.5795 - val_mse: 1.5795\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 1.53074\n",
      "Epoch 259/300\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 3.7793 - mse: 3.7793 - val_loss: 1.5729 - val_mse: 1.5729\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 1.53074\n",
      "Epoch 260/300\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 3.8056 - mse: 3.8056 - val_loss: 1.5574 - val_mse: 1.5574\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 1.53074\n",
      "Epoch 261/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 3.7742 - mse: 3.7742 - val_loss: 1.5683 - val_mse: 1.5683\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 1.53074\n",
      "Epoch 262/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 3.7863 - mse: 3.7863 - val_loss: 1.5483 - val_mse: 1.5483\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 1.53074\n",
      "Epoch 263/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 3.7724 - mse: 3.7724 - val_loss: 1.5441 - val_mse: 1.5441\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 1.53074\n",
      "Epoch 264/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 3.7349 - mse: 3.7349 - val_loss: 1.5613 - val_mse: 1.5613\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 1.53074\n",
      "Epoch 265/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 3.7324 - mse: 3.7324 - val_loss: 1.5577 - val_mse: 1.5577\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 1.53074\n",
      "Epoch 266/300\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 3.7196 - mse: 3.7196 - val_loss: 1.5494 - val_mse: 1.5494\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 1.53074\n",
      "Epoch 267/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 3.6995 - mse: 3.6995 - val_loss: 1.5423 - val_mse: 1.5423\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 1.53074\n",
      "Epoch 268/300\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 3.6835 - mse: 3.6835 - val_loss: 1.5380 - val_mse: 1.5380\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 1.53074\n",
      "Epoch 269/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 3.6800 - mse: 3.6800 - val_loss: 1.5699 - val_mse: 1.5699\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 1.53074\n",
      "Epoch 270/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 3.6786 - mse: 3.6786 - val_loss: 1.5403 - val_mse: 1.5403\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 1.53074\n",
      "Epoch 271/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 3.5960 - mse: 3.5960 - val_loss: 1.5655 - val_mse: 1.5655\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 1.53074\n",
      "Epoch 272/300\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 3.6211 - mse: 3.6211 - val_loss: 1.5314 - val_mse: 1.5314\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 1.53074\n",
      "Epoch 273/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 3.6032 - mse: 3.6032 - val_loss: 1.5587 - val_mse: 1.5587\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 1.53074\n",
      "Epoch 274/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 3.6025 - mse: 3.6025 - val_loss: 1.5537 - val_mse: 1.5537\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 1.53074\n",
      "Epoch 275/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 3.5869 - mse: 3.5869 - val_loss: 1.5490 - val_mse: 1.5490\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 1.53074\n",
      "Epoch 276/300\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 3.5658 - mse: 3.5658 - val_loss: 1.5551 - val_mse: 1.5551\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 1.53074\n",
      "Epoch 277/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 3.5620 - mse: 3.5620 - val_loss: 1.5360 - val_mse: 1.5360\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 1.53074\n",
      "Epoch 278/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 3.5688 - mse: 3.5688 - val_loss: 1.5380 - val_mse: 1.5380\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 1.53074\n",
      "Epoch 279/300\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 3.5081 - mse: 3.5081 - val_loss: 1.5460 - val_mse: 1.5460\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 1.53074\n",
      "Epoch 280/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 3.5209 - mse: 3.5209 - val_loss: 1.5352 - val_mse: 1.5352\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 1.53074\n",
      "Epoch 281/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 3.4878 - mse: 3.4878 - val_loss: 1.5531 - val_mse: 1.5531\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 1.53074\n",
      "Epoch 282/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 3.4978 - mse: 3.4978 - val_loss: 1.5444 - val_mse: 1.5444\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 1.53074\n",
      "Epoch 283/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 3.4792 - mse: 3.4792 - val_loss: 1.5469 - val_mse: 1.5469\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 1.53074\n",
      "Epoch 284/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 3.4595 - mse: 3.4595 - val_loss: 1.5283 - val_mse: 1.5283\n",
      "\n",
      "Epoch 00284: val_loss improved from 1.53074 to 1.52827, saving model to m0821.ckpt\n",
      "Epoch 285/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 3.4464 - mse: 3.4464 - val_loss: 1.5452 - val_mse: 1.5452\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 1.52827\n",
      "Epoch 286/300\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 3.4571 - mse: 3.4571 - val_loss: 1.5427 - val_mse: 1.5427\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 1.52827\n",
      "Epoch 287/300\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 3.4435 - mse: 3.4435 - val_loss: 1.5499 - val_mse: 1.5499\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 1.52827\n",
      "Epoch 288/300\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 3.3973 - mse: 3.3973 - val_loss: 1.5590 - val_mse: 1.5590\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 1.52827\n",
      "Epoch 289/300\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 3.4253 - mse: 3.4253 - val_loss: 1.5493 - val_mse: 1.5493\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 1.52827\n",
      "Epoch 290/300\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 3.3959 - mse: 3.3959 - val_loss: 1.5458 - val_mse: 1.5458\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 1.52827\n",
      "Epoch 291/300\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 3.3779 - mse: 3.3779 - val_loss: 1.5415 - val_mse: 1.5415\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 1.52827\n",
      "Epoch 292/300\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 3.3562 - mse: 3.3562 - val_loss: 1.5468 - val_mse: 1.5468\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 1.52827\n",
      "Epoch 293/300\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 3.3595 - mse: 3.3595 - val_loss: 1.5611 - val_mse: 1.5611\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 1.52827\n",
      "Epoch 294/300\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 3.3509 - mse: 3.3509 - val_loss: 1.5258 - val_mse: 1.5258\n",
      "\n",
      "Epoch 00294: val_loss improved from 1.52827 to 1.52577, saving model to m0821.ckpt\n",
      "Epoch 295/300\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 3.3116 - mse: 3.3116 - val_loss: 1.5480 - val_mse: 1.5480\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 1.52577\n",
      "Epoch 296/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 3.3585 - mse: 3.3585 - val_loss: 1.5414 - val_mse: 1.5414\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 1.52577\n",
      "Epoch 297/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 3.2822 - mse: 3.2822 - val_loss: 1.5452 - val_mse: 1.5452\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 1.52577\n",
      "Epoch 298/300\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 3.2981 - mse: 3.2981 - val_loss: 1.5447 - val_mse: 1.5447\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 1.52577\n",
      "Epoch 299/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 3.3035 - mse: 3.3035 - val_loss: 1.5431 - val_mse: 1.5431\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 1.52577\n",
      "Epoch 300/300\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 3.2741 - mse: 3.2741 - val_loss: 1.5449 - val_mse: 1.5449\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 1.52577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3354a5ebe0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,\n",
    "          validation_data=(x_test,y_test),\n",
    "          epochs=300, batch_size = 512,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "398040cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 56)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 112)          6384        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 112)          448         dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 112)          0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_9 (TFOpLam (None, 112)          0           dropout_12[0][0]                 \n",
      "                                                                 dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 60)           6780        tf.__operators__.add_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 60)           240         dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 60)           0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_10 (TFOpLa (None, 60)           0           dropout_13[0][0]                 \n",
      "                                                                 dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 30)           1830        tf.__operators__.add_10[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 30)           120         dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 30)           0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_11 (TFOpLa (None, 30)           0           dropout_14[0][0]                 \n",
      "                                                                 dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 30)           0           tf.__operators__.add_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 14)           434         dropout_15[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 16,236\n",
      "Trainable params: 15,832\n",
      "Non-trainable params: 404\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model = get_model()\n",
    "new_model.load_weights('m0821.ckpt')\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48ca0a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(loss = 'mse',\n",
    "             optimizer = 'adam',\n",
    "             metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb08997a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/248 [==============================] - 1s 3ms/step - loss: 1.5258 - mse: 1.5258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5257716178894043, 1.5257716178894043]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c565852",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = new_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c95cddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lg_nrmse(gt, preds):\n",
    "    # 각 Y Feature별 NRMSE 총합\n",
    "    # Y_01 ~ Y_08 까지 20% 가중치 부여\n",
    "    all_nrmse = []\n",
    "    for idx in range(14): # ignore 'ID'\n",
    "        rmse = metrics.mean_squared_error(gt[:,idx], preds[:,idx], squared=False)\n",
    "        nrmse = rmse/np.mean(np.abs(gt[:,idx]))\n",
    "        all_nrmse.append(nrmse)\n",
    "    score = 1.2 * np.sum(all_nrmse[:8]) + 1.0 * np.sum(all_nrmse[8:14])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf1e78ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0152360933191997"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg_nrmse(y_test,predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
